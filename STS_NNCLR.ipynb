{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E4mouVaLaHMk",
        "0NV5nSjlaMJF",
        "ZtNacDXJ5Rbw",
        "Cnb_7Yr15DYL",
        "26GYXJ114wvz",
        "-Jt68MTJ45B7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14e5d2ae349e4c25af37b5abcc473a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d558987c23a44eabb527a959d38a64f8",
              "IPY_MODEL_a4a23725897d4f5eb360ff932ebe6896",
              "IPY_MODEL_45e08f60dfe047c58bb5527103beffb1"
            ],
            "layout": "IPY_MODEL_9cf5b79609694ebc98766b77973ca72b"
          }
        },
        "d558987c23a44eabb527a959d38a64f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03fcd9bd0b74482e83a527eaa444e36c",
            "placeholder": "​",
            "style": "IPY_MODEL_40a6607c716c459ba68d5e12cb08d64f",
            "value": "100%"
          }
        },
        "a4a23725897d4f5eb360ff932ebe6896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57a3d4bad22440308b364de12f11a897",
            "max": 40794454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1a08842b3a84d4b8316eb3c8198ef30",
            "value": 40794454
          }
        },
        "45e08f60dfe047c58bb5527103beffb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_599eda2f821b4cfeaaa4905886c702df",
            "placeholder": "​",
            "style": "IPY_MODEL_15b997b89f544688ba4487275bec7ca7",
            "value": " 40.8M/40.8M [00:01&lt;00:00, 41.7MB/s]"
          }
        },
        "9cf5b79609694ebc98766b77973ca72b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03fcd9bd0b74482e83a527eaa444e36c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a6607c716c459ba68d5e12cb08d64f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57a3d4bad22440308b364de12f11a897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a08842b3a84d4b8316eb3c8198ef30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "599eda2f821b4cfeaaa4905886c702df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15b997b89f544688ba4487275bec7ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c58e60995ae149a586e50c3f40455111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c873ffdb730a41bfae50f27a4e43a6ed",
              "IPY_MODEL_94169ac40d71436d903e9b182d4b6b2e",
              "IPY_MODEL_7f8e499e37d149439a4ae952627fb508"
            ],
            "layout": "IPY_MODEL_524a2a5ef535485d8909fedb550e7aa7"
          }
        },
        "c873ffdb730a41bfae50f27a4e43a6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65299391f4124130977b9f19a6b0d523",
            "placeholder": "​",
            "style": "IPY_MODEL_b6baf9a9f21246729e8824542f118f48",
            "value": "100%"
          }
        },
        "94169ac40d71436d903e9b182d4b6b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad31bcd7949345d49ff390e3a9b35d2b",
            "max": 392336,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa8823ed747e4ad49d4975a35ef671fd",
            "value": 392336
          }
        },
        "7f8e499e37d149439a4ae952627fb508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f865ce4d886a4d94be698a50ae0751bf",
            "placeholder": "​",
            "style": "IPY_MODEL_ef5632a1cdfc4931b894f72334a73208",
            "value": " 392k/392k [00:00&lt;00:00, 4.35MB/s]"
          }
        },
        "524a2a5ef535485d8909fedb550e7aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65299391f4124130977b9f19a6b0d523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6baf9a9f21246729e8824542f118f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad31bcd7949345d49ff390e3a9b35d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa8823ed747e4ad49d4975a35ef671fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f865ce4d886a4d94be698a50ae0751bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5632a1cdfc4931b894f72334a73208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "E4mouVaLaHMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip -q install sentence_transformers\n",
        "! pip -q install torchsummaryX\n",
        "! pip -q install wandb"
      ],
      "metadata": {
        "id": "SDdGsgJmL3Zr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4QyO_oukw8Vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb095010-6461-417c-a3ff-6a6ba79ff890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import gzip\n",
        "import random\n",
        "\n",
        "from sentence_transformers import models, losses\n",
        "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util, InputExample\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
        "from transformers import BertModel, BertTokenizer, BertConfig\n",
        "from torchsummaryX import summary\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers.models.bert.modeling_bert import BertEmbeddings\n",
        "import wandb\n",
        "import json\n",
        "from typing import Optional\n",
        "\n",
        "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'model_name': 'bert-base-uncased',\n",
        "    'augmentation_type_1': 'shuffle',\n",
        "    'augmentation_type_2': 'cutoff',\n",
        "    'BATCH_SIZE': 96,\n",
        "    'models': os.getcwd() + '/model',\n",
        "    'data_path': './downstream',\n",
        "    'cutoff_direction': 'column',\n",
        "    'cutoff_rate': 0.2,\n",
        "    'lr': 2e-5,\n",
        "    'weight_decay': 1e-4,\n",
        "    'temperature': 0.05,\n",
        "    'LARGE_NUM': 1e9,\n",
        "    'hidden_norm': True,\n",
        "    'total_epochs': 20,\n",
        "    'max_length': 64,\n",
        "    'concatenated_out_len': 2304,\n",
        "    'num_labels': 3,\n",
        "    'mixup_rate': 0.4\n",
        "}"
      ],
      "metadata": {
        "id": "Kl0_REObkqZh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "0NV5nSjlaMJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/yym6472/ConSERT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK8gPy5jxAn6",
        "outputId": "a2a6aaf9-5bfc-4db2-c5bc-c4fa4cd985a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ConSERT'...\n",
            "remote: Enumerating objects: 475, done.\u001b[K\n",
            "remote: Counting objects: 100% (475/475), done.\u001b[K\n",
            "remote: Compressing objects: 100% (333/333), done.\u001b[K\n",
            "remote: Total 475 (delta 166), reused 444 (delta 139), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (475/475), 1.13 MiB | 5.65 MiB/s, done.\n",
            "Resolving deltas: 100% (166/166), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! bash /content/ConSERT/data/get_transfer_data.bash"
      ],
      "metadata": {
        "id": "K543hec0uixH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -q -r downstream.zip /content/downstream/"
      ],
      "metadata": {
        "id": "V_bNhqh7LEsl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
        "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
        "if not os.path.exists(nli_dataset_path):\n",
        "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
        "if not os.path.exists(sts_dataset_path):\n",
        "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "14e5d2ae349e4c25af37b5abcc473a87",
            "d558987c23a44eabb527a959d38a64f8",
            "a4a23725897d4f5eb360ff932ebe6896",
            "45e08f60dfe047c58bb5527103beffb1",
            "9cf5b79609694ebc98766b77973ca72b",
            "03fcd9bd0b74482e83a527eaa444e36c",
            "40a6607c716c459ba68d5e12cb08d64f",
            "57a3d4bad22440308b364de12f11a897",
            "d1a08842b3a84d4b8316eb3c8198ef30",
            "599eda2f821b4cfeaaa4905886c702df",
            "15b997b89f544688ba4487275bec7ca7",
            "c58e60995ae149a586e50c3f40455111",
            "c873ffdb730a41bfae50f27a4e43a6ed",
            "94169ac40d71436d903e9b182d4b6b2e",
            "7f8e499e37d149439a4ae952627fb508",
            "524a2a5ef535485d8909fedb550e7aa7",
            "65299391f4124130977b9f19a6b0d523",
            "b6baf9a9f21246729e8824542f118f48",
            "ad31bcd7949345d49ff390e3a9b35d2b",
            "aa8823ed747e4ad49d4975a35ef671fd",
            "f865ce4d886a4d94be698a50ae0751bf",
            "ef5632a1cdfc4931b894f72334a73208"
          ]
        },
        "id": "pdGhWOwVLmDt",
        "outputId": "413cab1a-7545-4ecd-dec1-b15ae0de9014"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/40.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14e5d2ae349e4c25af37b5abcc473a87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/392k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c58e60995ae149a586e50c3f40455111"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.mkdir(os.getcwd() + '/model')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "b-ztKI3rtIep"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read STS"
      ],
      "metadata": {
        "id": "ZtNacDXJ5Rbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sts12(need_label = False, use_all_unsupervised_texts=True, no_pair=True):\n",
        "    dataset_names = [\"MSRpar\", \"MSRvid\", \"SMTeuroparl\", \"surprise.OnWN\", \"surprise.SMTnews\"]\n",
        "    return load_sts(need_label, \"12\", dataset_names, no_pair=no_pair)\n",
        "    \n",
        "def load_sts13(need_label = False, use_all_unsupervised_texts=True, no_pair=True):\n",
        "    dataset_names = [\"headlines\", \"OnWN\", \"FNWN\"]\n",
        "    return load_sts(need_label, \"13\", dataset_names, no_pair=no_pair)\n",
        "\n",
        "def load_sts14(need_label = False, use_all_unsupervised_texts=True, no_pair=True):\n",
        "    dataset_names = [\"images\", \"OnWN\", \"tweet-news\", \"deft-news\", \"deft-forum\", \"headlines\"]\n",
        "    return load_sts(need_label, \"14\", dataset_names, no_pair=no_pair)\n",
        "\n",
        "def load_sts15(need_label = False, use_all_unsupervised_texts=True, no_pair=True):\n",
        "    dataset_names = [\"answers-forums\", \"answers-students\", \"belief\", \"headlines\", \"images\"]\n",
        "    return load_sts(need_label, \"15\", dataset_names, no_pair=no_pair)\n",
        "\n",
        "def load_sts16(need_label = False, use_all_unsupervised_texts=True, no_pair=True):\n",
        "    dataset_names = [\"answer-answer\", \"headlines\", \"plagiarism\", \"postediting\", \"question-question\"]\n",
        "    return load_sts(need_label, \"16\", dataset_names, no_pair=no_pair)\n",
        "\n",
        "def load_sts(need_label, year, dataset_names, no_pair=False):\n",
        "    \n",
        "    all_samples = []\n",
        "    sts_data_path = f\"{config['data_path']}/STS/STS{year}-en-test\"\n",
        "    \n",
        "    for dataset_name in dataset_names:\n",
        "        input_file = os.path.join(sts_data_path, f\"STS.input.{dataset_name}.txt\")\n",
        "        label_file = os.path.join(sts_data_path, f\"STS.gs.{dataset_name}.txt\")\n",
        "        sub_samples = load_paired_samples(need_label, input_file, label_file, no_pair=no_pair)\n",
        "        all_samples.extend(sub_samples)\n",
        "    \n",
        "    return all_samples\n",
        "\n",
        "def load_paired_samples(need_label, input_file, label_file, scale=5.0, no_pair=False):\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    with open(input_file, \"r\") as f:\n",
        "        input_lines = f.readlines()\n",
        "\n",
        "    label_lines = [None]*len(input_lines)\n",
        "    if label_file:\n",
        "        with open(label_file, 'r') as labels:\n",
        "            label_lines = labels.readlines()\n",
        "\n",
        "    # Filtering out lines without labels\n",
        "    if need_label:\n",
        "        new_input_lines, new_label_lines = [], []\n",
        "        for idx in range(len(label_lines)):\n",
        "            label_lines[idx] = label_lines[idx].strip()\n",
        "            if label_lines[idx]:\n",
        "                new_input_lines.append(input_lines[idx])\n",
        "                new_label_lines.append(label_lines[idx].strip())\n",
        "        input_lines = new_input_lines\n",
        "        label_lines = new_label_lines\n",
        "\n",
        "    # Parsing text file for sentence and label\n",
        "    for input_line, label_line in zip(input_lines, label_lines):\n",
        "        sentences = input_line.split(\"\\t\")\n",
        "            \n",
        "        if len(sentences)==2:\n",
        "            sent1, sent2 = sentences\n",
        "        else:\n",
        "            sent1, sent2 = sentences[0], None\n",
        "\n",
        "        if need_label:\n",
        "            samples.append(InputExample(texts=[sent1, sent2], label=float(label_line)/scale))\n",
        "\n",
        "        else:\n",
        "            if no_pair:\n",
        "                samples.append(InputExample(texts=[sent1]))\n",
        "                if sent2:\n",
        "                    samples.append(InputExample(texts=[sent2]))\n",
        "            else:\n",
        "                samples.append(InputExample(texts=[sent1, sent2]))\n",
        "    return samples\n",
        "\n",
        "\n",
        "def load_stsbenchmark(need_label=False, use_all_unsupervised_texts=True, no_pair=True):\n",
        "\n",
        "    all_samples = []\n",
        "    if use_all_unsupervised_texts:\n",
        "        splits = [\"train\", \"dev\", \"test\"]\n",
        "    else:\n",
        "        splits = [\"test\"]\n",
        "    \n",
        "    for split in splits:\n",
        "        sts_benchmark_data_path = f\"{config['data_path']}/STS/STSBenchmark/sts-{split}.csv\"\n",
        "        \n",
        "        samples = []\n",
        "        with open(sts_benchmark_data_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                _, _, _, _, label, sent1, sent2 = line.split(\"\\t\")\n",
        "                if need_label:\n",
        "                    samples.append(InputExample(texts=[sent1, sent2], label=float(label) / 5.0))\n",
        "                else:\n",
        "                    if no_pair:\n",
        "                        samples.append(InputExample(texts=[sent1]))\n",
        "                        samples.append(InputExample(texts=[sent2]))\n",
        "                    else:\n",
        "                        samples.append(InputExample(texts=[sent1, sent2]))\n",
        "        all_samples.extend(samples)\n",
        "    \n",
        "    return all_samples\n",
        "\n",
        "def load_sickr(need_label=False, use_all_unsupervised_texts=True, no_pair=True):\n",
        "    \n",
        "    all_samples = []\n",
        "    if use_all_unsupervised_texts:\n",
        "        splits = [\"train\", \"trial\", \"test_annotated\"]\n",
        "    else:\n",
        "        splits = [\"test_annotated\"]\n",
        "\n",
        "    for split in splits:\n",
        "        samples = []\n",
        "        sick_data_path = f\"{config['data_path']}/SICK/SICK_{split}.txt\"\n",
        "        \n",
        "        with open(sick_data_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        for line in lines[1:]:\n",
        "            line = line.strip()\n",
        "            _, sent1, sent2, label, _ = line.split(\"\\t\")\n",
        "            \n",
        "            if need_label:\n",
        "                samples.append(InputExample(texts=[sent1, sent2], label=float(label) / 5.0))\n",
        "            else:\n",
        "                if no_pair:\n",
        "                    samples.append(InputExample(texts=[sent1]))\n",
        "                    samples.append(InputExample(texts=[sent2]))\n",
        "                else:\n",
        "                    samples.append(InputExample(texts=[sent1, sent2]))\n",
        "        all_samples.extend(samples)\n",
        "    \n",
        "    return all_samples"
      ],
      "metadata": {
        "id": "el1XzvO_U1VA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_sts(year, dataset_names):\n",
        "    print(f\"Evaluation on STS{year} dataset\")\n",
        "    sts_data_path = f\"./downstream/STS/STS{year}-en-test\"\n",
        "    \n",
        "    all_samples = []\n",
        "    for dataset_name in dataset_names:\n",
        "        input_file = os.path.join(sts_data_path, f\"STS.input.{dataset_name}.txt\")\n",
        "        label_file = os.path.join(sts_data_path, f\"STS.gs.{dataset_name}.txt\")\n",
        "        sub_samples = load_paired_samples(True, input_file, label_file)\n",
        "        all_samples.extend(sub_samples)\n",
        "    print(f\"Loaded examples from STS{year} dataset, total {len(all_samples)} examples\")\n",
        "    return all_samples\n",
        "\n",
        "def eval_sts12():\n",
        "    dataset_names = [\"MSRpar\", \"MSRvid\", \"SMTeuroparl\", \"surprise.OnWN\", \"surprise.SMTnews\"]\n",
        "    return eval_sts(\"12\", dataset_names)\n",
        "    \n",
        "def eval_sts13():\n",
        "    dataset_names = [\"headlines\", \"OnWN\", \"FNWN\"]\n",
        "    return eval_sts(\"13\", dataset_names)\n",
        "\n",
        "def eval_sts14():\n",
        "    dataset_names = [\"images\", \"OnWN\", \"tweet-news\", \"deft-news\", \"deft-forum\", \"headlines\"]\n",
        "    return eval_sts(\"14\", dataset_names)\n",
        "\n",
        "def eval_sts15():\n",
        "    dataset_names = [\"answers-forums\", \"answers-students\", \"belief\", \"headlines\", \"images\"]\n",
        "    return eval_sts(\"15\", dataset_names)\n",
        "\n",
        "def eval_sts16():\n",
        "    dataset_names = [\"answer-answer\", \"headlines\", \"plagiarism\", \"postediting\", \"question-question\"]\n",
        "    return eval_sts(\"16\", dataset_names)\n",
        "\n",
        "def eval_stsbenchmark():\n",
        "    print(\"Evaluation on STSBenchmark dataset\")\n",
        "    sts_benchmark_data_path = \"./downstream/STS/STSBenchmark/sts-test.csv\"\n",
        "    with open(sts_benchmark_data_path, \"r\") as f:\n",
        "        lines = [line.strip() for line in f if line.strip()]\n",
        "    samples = []\n",
        "    for line in lines:\n",
        "        _, _, _, _, label, sent1, sent2 = line.split(\"\\t\")\n",
        "        samples.append(InputExample(texts=[sent1, sent2], label=float(label) / 5.0))\n",
        "    print(f\"Loaded examples from STSBenchmark dataset, total {len(samples)} examples\")\n",
        "    return samples\n",
        "\n",
        "def eval_sickr():\n",
        "    print(\"Evaluation on SICK (relatedness) dataset\")\n",
        "    sick_data_path = \"./downstream/SICK/SICK_test_annotated.txt\"\n",
        "    with open(sick_data_path, \"r\") as f:\n",
        "        lines = [line.strip() for line in f if line.strip()]\n",
        "    samples = []\n",
        "    for line in lines[1:]:\n",
        "        _, sent1, sent2, label, _ = line.split(\"\\t\")\n",
        "        samples.append(InputExample(texts=[sent1, sent2], label=float(label) / 5.0))\n",
        "    print(f\"Loaded examples from SICK dataset, total {len(samples)} examples\")\n",
        "    \n",
        "    return samples"
      ],
      "metadata": {
        "id": "YwdxKrMl11-k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = BertModel.from_pretrained('bert-base-uncased', hidden_dropout_prob=0, attention_probs_dropout_prob=0)\n",
        "encoder.model_max_len=config['max_length']\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', model_max_length=config['max_length'])"
      ],
      "metadata": {
        "id": "PheHWBb_74r4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0dba8e7-1190-45af-ad0a-0fbd60a32df8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "Cnb_7Yr15DYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class STSDatasetUnsupervised(Dataset):\n",
        "    def __init__(self):\n",
        "        super(STSDatasetUnsupervised, self).__init__()\n",
        "\n",
        "        sts_data = []\n",
        "        sts_data_12 = load_sts12()\n",
        "        sts_data_13 = load_sts13()\n",
        "        sts_data_14 = load_sts14()\n",
        "        sts_data_15 = load_sts15()\n",
        "        sts_data_16 = load_sts16()\n",
        "        stsb = load_stsbenchmark()\n",
        "        sickr = load_sickr()\n",
        "        \n",
        "        sts_data.extend(sts_data_12)\n",
        "        sts_data.extend(sts_data_13)\n",
        "        sts_data.extend(sts_data_14)\n",
        "        sts_data.extend(sts_data_15)\n",
        "        sts_data.extend(sts_data_16)\n",
        "        sts_data.extend(stsb)\n",
        "        sts_data.extend(sickr)\n",
        "\n",
        "        self.dataset = sts_data\n",
        "        self.length = len(self.dataset)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index]\n",
        "\n",
        "    def collate(batch):\n",
        "        num_texts = len(batch[0].texts)\n",
        "        texts = []\n",
        "\n",
        "        for example in batch:\n",
        "            texts.append(example.texts[0])\n",
        "        return tokenizer.batch_encode_plus(texts, padding='max_length', return_tensors='pt', truncation=True)"
      ],
      "metadata": {
        "id": "MDcWiAt--ENm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class STSDatasetUnsupervisedVal(Dataset):\n",
        "    def __init__(self):\n",
        "        super(STSDatasetUnsupervisedVal, self).__init__()\n",
        "        self.dataset = []\n",
        "        sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
        "        with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
        "            reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "            for row in reader:\n",
        "                if row['split'] == 'dev':\n",
        "                    score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
        "                    self.dataset.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
        "        \n",
        "        self.length = len(self.dataset)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index]\n",
        "\n",
        "    def collate(batch):\n",
        "        num_texts = len(batch[0].texts)\n",
        "        texts1, texts2 = [], []\n",
        "        labels = []\n",
        "\n",
        "        for example in batch:\n",
        "            texts1.append(example.texts[0])\n",
        "            texts2.append(example.texts[1])\n",
        "            labels.append(example.label)\n",
        "\n",
        "        return (tokenizer.batch_encode_plus(texts1, padding='max_length', return_tensors='pt', truncation=True),\n",
        "        tokenizer.batch_encode_plus(texts2, padding='max_length', return_tensors='pt', truncation=True),\n",
        "        labels)"
      ],
      "metadata": {
        "id": "1OCg-zpQSHvJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class STSDatasetUnsupervisedTest(Dataset):\n",
        "    def __init__(self):\n",
        "        super(STSDatasetUnsupervisedTest, self).__init__()\n",
        "\n",
        "        sts_data = []\n",
        "        sts_data_12 = eval_sts12()\n",
        "        sts_data_13 = eval_sts13()\n",
        "        sts_data_14 = eval_sts14()\n",
        "        sts_data_15 = eval_sts15()\n",
        "        sts_data_16 = eval_sts16()\n",
        "        stsb = eval_stsbenchmark()\n",
        "        sickr = eval_sickr()\n",
        "        \n",
        "        sts_data.extend(sts_data_12)\n",
        "        sts_data.extend(sts_data_13)\n",
        "        sts_data.extend(sts_data_14)\n",
        "        sts_data.extend(sts_data_15)\n",
        "        sts_data.extend(sts_data_16)\n",
        "        sts_data.extend(stsb)\n",
        "        sts_data.extend(sickr)\n",
        "\n",
        "        self.dataset = sts_data\n",
        "        self.length = len(self.dataset)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index]\n",
        "\n",
        "    def collate(batch):\n",
        "        num_texts = len(batch[0].texts)\n",
        "        texts1, texts2 = [], []\n",
        "        labels = []\n",
        "\n",
        "        for example in batch:\n",
        "            texts1.append(example.texts[0])\n",
        "            texts2.append(example.texts[1])\n",
        "            labels.append(example.label)\n",
        "\n",
        "        return (tokenizer.batch_encode_plus(texts1, padding='max_length', return_tensors='pt', truncation=True),\n",
        "        tokenizer.batch_encode_plus(texts2, padding='max_length', return_tensors='pt', truncation=True),\n",
        "        labels)"
      ],
      "metadata": {
        "id": "Sp4gcQWJcTKM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unsupervised_dataset = STSDatasetUnsupervised()\n",
        "unsupervised_dataset_val = STSDatasetUnsupervisedVal()\n",
        "unsupervised_dataset_test = STSDatasetUnsupervisedTest()\n",
        "train_dataloader = DataLoader(unsupervised_dataset, shuffle = True, batch_size=config['BATCH_SIZE'], collate_fn=STSDatasetUnsupervised.collate)\n",
        "val_dataloader = DataLoader(unsupervised_dataset_val, shuffle = False, batch_size=config['BATCH_SIZE'], collate_fn=STSDatasetUnsupervisedVal.collate)\n",
        "test_dataloader = DataLoader(unsupervised_dataset_test, shuffle = False, batch_size=config['BATCH_SIZE'], collate_fn=STSDatasetUnsupervisedTest.collate)"
      ],
      "metadata": {
        "id": "oyBkd3b0cZez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9daef9-27e2-4d0e-d7fb-a0d4736546b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on STS12 dataset\n",
            "Loaded examples from STS12 dataset, total 3108 examples\n",
            "Evaluation on STS13 dataset\n",
            "Loaded examples from STS13 dataset, total 1500 examples\n",
            "Evaluation on STS14 dataset\n",
            "Loaded examples from STS14 dataset, total 3750 examples\n",
            "Evaluation on STS15 dataset\n",
            "Loaded examples from STS15 dataset, total 3000 examples\n",
            "Evaluation on STS16 dataset\n",
            "Loaded examples from STS16 dataset, total 1186 examples\n",
            "Evaluation on STSBenchmark dataset\n",
            "Loaded examples from STSBenchmark dataset, total 1379 examples\n",
            "Evaluation on SICK (relatedness) dataset\n",
            "Loaded examples from SICK dataset, total 4927 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(unsupervised_dataset.length)\n",
        "print(unsupervised_dataset_val.length)\n",
        "print(unsupervised_dataset_test.length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8FVjKwVPzO3",
        "outputId": "f4b29c35-9a85-4a40-adef-7975f5e3be48"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89192\n",
            "1500\n",
            "18850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (x1, x2, y) in enumerate(val_dataloader):\n",
        "    print(x1['input_ids'].shape, x2['input_ids'].shape, len(y))\n",
        "    break"
      ],
      "metadata": {
        "id": "dcYPhms6MNak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de594a2-8a38-4781-d3ef-2ee4e6c5b364"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([96, 64]) torch.Size([96, 64]) 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, x in enumerate(train_dataloader):\n",
        "    break"
      ],
      "metadata": {
        "id": "tvcxbNRxx1L4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "\n",
        "# global feature_queue\n",
        "# gc.collect()\n",
        "# feature_queue = torch.nn.functional.normalize(torch.randn(size=(9600, 768), requires_grad=False), p=2.0, dim = 1).to(device)\n",
        "# f_q_temp = torch.clone(feature_queue)\n",
        "# def mean_pooling(model_output, attention_mask):\n",
        "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "# def append_to_queue(x, i):\n",
        "#     global feature_queue\n",
        "#     temp_embeddings = encoder(input_ids=x['input_ids'].to(device), attention_mask=x['attention_mask'].to(device))\n",
        "#     temp_embeddings = mean_pooling(temp_embeddings, x['attention_mask'].to(device))\n",
        "#     temp_embeddings = torch.nn.functional.normalize(temp_embeddings, p=2, dim=-1)\n",
        "#     feature_queue = torch.concat([feature_queue[config['BATCH_SIZE'] - 1:], temp_embeddings], axis=0)\n",
        "\n",
        "# for i, x in enumerate(train_dataloader):\n",
        "#     if i == 0:\n",
        "#         to_find_embeddings = encoder(input_ids=x['input_ids'].to(device), attention_mask=x['attention_mask'].to(device))\n",
        "#         to_find_embeddings = mean_pooling(to_find_embeddings, x['attention_mask'].to(device))\n",
        "#         to_find_embeddings = torch.nn.functional.normalize(to_find_embeddings, p=2, dim=-1)\n",
        "#         continue\n",
        "#     if i == 100:\n",
        "#         break\n",
        "#     print(i)\n",
        "\n",
        "# support_similarities = torch.matmul(to_find_embeddings, feature_queue.T)\n",
        "# indices = torch.argmax(support_similarities, dim=-1)\n",
        "# similarities = torch.max(support_similarities, dim=1)"
      ],
      "metadata": {
        "id": "9H4J5s2BL3QF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(indices)"
      ],
      "metadata": {
        "id": "aQD8nCMIrCa6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(similarities.values.sum())"
      ],
      "metadata": {
        "id": "em4i60jrt5C-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# support_similarities = torch.matmul(to_find_embeddings, f_q_temp.T)\n",
        "# indices = torch.argmax(support_similarities, dim=-1)\n",
        "# similarities = torch.max(support_similarities, dim=1)"
      ],
      "metadata": {
        "id": "oHgRCOZdVIxP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(similarities.values.sum())"
      ],
      "metadata": {
        "id": "OmZfKRJ9Zuw8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(unsupervised_dataset[20000], unsupervised_dataset[20001], unsupervised_dataset[20002], unsupervised_dataset[20003])"
      ],
      "metadata": {
        "id": "AGbyQq84tsX1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(unsupervised_dataset[3116])"
      ],
      "metadata": {
        "id": "1Mg9sZdZt0Fn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "gWDnYBPz4p6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NTXENT(nn.Module):\n",
        "    def __init__(self, temperature, LARGE_NUM, hidden_norm):\n",
        "        self.temperature = temperature\n",
        "        self.LARGE_NUM = LARGE_NUM\n",
        "        self.hidden_norm = hidden_norm\n",
        "        self.batch_size = config['BATCH_SIZE']\n",
        "\n",
        "    def __call__(self, out1, out2):\n",
        "        if self.hidden_norm:\n",
        "            out1 = torch.nn.functional.normalize(out1, p=2, dim=-1)\n",
        "            out2 = torch.nn.functional.normalize(out2, p=2, dim=-1)\n",
        "\n",
        "        out = torch.cat([out1, out2], dim=0)\n",
        "        n_samples = len(out)\n",
        "\n",
        "        # Full similarity matrix\n",
        "        cov = torch.mm(out, out.t().contiguous())\n",
        "        sim = torch.exp(cov / self.temperature)\n",
        "\n",
        "        # Negative similarity\n",
        "        mask = ~torch.eye(n_samples, device=device).bool()\n",
        "        neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
        "\n",
        "        # Positive similarity :\n",
        "        pos = torch.exp(torch.sum(out1 * out2, dim=-1) / self.temperature)\n",
        "        pos = torch.cat([pos, pos], dim=0)\n",
        "\n",
        "        loss = -torch.log(pos / neg).mean()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "KqKBkmDqxMeE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "26GYXJ114wvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Augmentation(nn.Module):\n",
        "    def __init__(self, direction, rate):\n",
        "        super(Augmentation, self).__init__()\n",
        "        # self.bert_embeddings = BertEmbeddings(bert_config)\n",
        "        self.direction = direction\n",
        "        self.rate = rate\n",
        "\n",
        "    def forward(self, sentence_feature, augmentation_type, embeddings):\n",
        "        input_ids, token_type_ids, attention_mask = sentence_feature['input_ids'], sentence_feature['token_type_ids'], sentence_feature['attention_mask']\n",
        "        bs, seq_len = input_ids.shape\n",
        "        input_ids, token_type_ids, attention_mask = input_ids.to(device), token_type_ids.to(device), attention_mask.to(device)\n",
        "        position_ids = torch.arange(seq_len).expand((bs, -1))[:, :seq_len].to(device=input_ids.device)\n",
        "\n",
        "        if not self.training or augmentation_type is None:\n",
        "            inputs_embeds = embeddings.word_embeddings(input_ids)\n",
        "            token_type_embeddings = embeddings.token_type_embeddings(token_type_ids)\n",
        "            position_embeddings = embeddings.position_embeddings(position_ids)\n",
        "\n",
        "            embedding_output = inputs_embeds + position_embeddings + token_type_embeddings\n",
        "            del inputs_embeds, token_type_embeddings, position_embeddings\n",
        "\n",
        "            return embedding_output, attention_mask\n",
        "\n",
        "        if augmentation_type == 'shuffle':\n",
        "            # Shuffle\n",
        "            position_ids = self._replace_position_ids(input_ids, position_ids, attention_mask)\n",
        "\n",
        "            inputs_embeds = embeddings.word_embeddings(input_ids)\n",
        "            token_type_embeddings = embeddings.token_type_embeddings(token_type_ids)\n",
        "            position_embeddings = embeddings.position_embeddings(position_ids)\n",
        "\n",
        "            embedding_output = inputs_embeds + position_embeddings + token_type_embeddings\n",
        "            del inputs_embeds, token_type_embeddings, position_embeddings\n",
        "\n",
        "            return embedding_output, attention_mask\n",
        "            \n",
        "        elif augmentation_type == 'cutoff':\n",
        "\n",
        "            inputs_embeds = embeddings.word_embeddings(input_ids)\n",
        "            token_type_embeddings = embeddings.token_type_embeddings(token_type_ids)\n",
        "            position_embeddings = embeddings.position_embeddings(position_ids)\n",
        "\n",
        "            embedding_output = inputs_embeds + position_embeddings + token_type_embeddings\n",
        "\n",
        "            del inputs_embeds, token_type_embeddings, position_embeddings\n",
        "\n",
        "            return self.apply_cutoff(embedding_output, attention_mask, self.direction, self.rate)\n",
        "\n",
        "    def apply_cutoff(self, embedding_output, attention_mask, direction, rate):\n",
        "        bs, seq_len, emb_size = embedding_output.shape\n",
        "        cutoff_embeddings = []\n",
        "        for batch_id in range(bs):\n",
        "            sample_embedding = embedding_output[batch_id]\n",
        "            sample_mask = attention_mask[batch_id]\n",
        "            if direction == \"row\":\n",
        "                num_dimensions = sample_mask.sum().int().item()  # number of tokens\n",
        "                dim_index = 0\n",
        "            elif direction == \"column\":\n",
        "                num_dimensions = emb_size  # number of features\n",
        "                dim_index = 1\n",
        "            elif direction == \"random\":\n",
        "                num_dimensions = sample_mask.sum().int().item() * emb_size\n",
        "                dim_index = 0\n",
        "            else:\n",
        "                raise ValueError(f\"direction should be either row or column, but got {direction}\")\n",
        "\n",
        "            num_cutoff_indexes = int(num_dimensions * rate)\n",
        "\n",
        "            if num_cutoff_indexes < 0 or num_cutoff_indexes > num_dimensions:\n",
        "                raise ValueError(f\"number of cutoff dimensions should be in (0, {num_dimensions}), but got {num_cutoff_indexes}\")\n",
        "            \n",
        "            indexes = list(range(num_dimensions))\n",
        "            random.shuffle(indexes)\n",
        "            cutoff_indexes = indexes[:num_cutoff_indexes]\n",
        "\n",
        "            if direction == \"random\":\n",
        "                sample_embedding = sample_embedding.reshape(-1)\n",
        "            \n",
        "            cutoff_embedding = torch.index_fill(sample_embedding, dim_index, torch.tensor(cutoff_indexes, dtype=torch.long).to(device), 0.0)\n",
        "            \n",
        "            if direction == \"random\":\n",
        "                cutoff_embedding = cutoff_embedding.reshape(seq_len, emb_size)\n",
        "                \n",
        "            cutoff_embeddings.append(cutoff_embedding.unsqueeze(0))\n",
        "        cutoff_embeddings = torch.cat(cutoff_embeddings, 0)\n",
        "\n",
        "        assert cutoff_embeddings.shape == embedding_output.shape, (cutoff_embeddings.shape, embedding_output.shape)\n",
        "        return cutoff_embeddings, attention_mask\n",
        "    \n",
        "    def _replace_position_ids(self, input_ids, position_ids, attention_mask):\n",
        "        bs, seq_len = input_ids.shape\n",
        "        \n",
        "        shuffled_pid = []\n",
        "        for batch_id in range(bs):\n",
        "            sample_pid = position_ids[batch_id]\n",
        "            sample_mask = attention_mask[batch_id]\n",
        "            num_tokens = sample_mask.sum().int().item()\n",
        "            indexes = list(range(num_tokens))\n",
        "            random.shuffle(indexes)\n",
        "            rest_indexes = list(range(num_tokens, seq_len))\n",
        "            total_indexes = indexes + rest_indexes\n",
        "            shuffled_pid.append(torch.index_select(sample_pid, 0, torch.tensor(total_indexes).to(device=input_ids.device)).unsqueeze(0))\n",
        "        return torch.cat(shuffled_pid, 0)"
      ],
      "metadata": {
        "id": "UhoSxWkbINhF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "encoder.model_max_len=config['max_length']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtJ633jUu_R9",
        "outputId": "f66b3878-7231-4d9a-c9f1-007aed5f4677"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "-Jt68MTJ45B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, input_dim_projection, hidden_dim_projection, output_dim_projection, hidden_dim_prediction, lamb, mixup):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        ##########################CHANGE THIS WHEN AUGMENTATION IS APPLIED\n",
        "        self.embedding = encoder.embeddings\n",
        "        self.encoder_layers = encoder.encoder\n",
        "        self.augmentation_module = Augmentation(config['cutoff_direction'], config['cutoff_rate'])\n",
        "        self.feature_queue = torch.nn.functional.normalize(torch.randn(size=(9600, 768), requires_grad=False), p=2.0, dim = 1).to(device)\n",
        "        self.lamb = lamb\n",
        "        self.mixup = mixup\n",
        "        \n",
        "\n",
        "    def forward(self, x1, x2=None):\n",
        "        if x2 is None:\n",
        "            x2 = x1\n",
        "        \n",
        "        embedding_output1, attention_mask1 = self.augmentation_module(x1, config['augmentation_type_1'], self.embedding.to(device))\n",
        "        embedding_output2, attention_mask2 = self.augmentation_module(x2, config['augmentation_type_2'], self.embedding.to(device))\n",
        "\n",
        "        extended_attention_mask1 = attention_mask1[:, None, None, :]\n",
        "        extended_attention_mask2 = attention_mask2[:, None, None, :]\n",
        "\n",
        "        model_output_1 = self.encoder_layers(embedding_output1, extended_attention_mask1)\n",
        "        model_output_2 = self.encoder_layers(embedding_output2, extended_attention_mask2)\n",
        "\n",
        "        if self.mixup:\n",
        "            model_output_1 = self.lamb * model_output_1  + (1-self.lamb) * model_output_2  \n",
        "            attention_mask1 = torch.max(extended_attention_mask1, extended_attention_mask2)\n",
        "\n",
        "\n",
        "        sentence_embedding_1 = self.mean_pooling(model_output_1, attention_mask1.to(device))\n",
        "        sentence_embedding_2 = self.mean_pooling(model_output_2, attention_mask2.to(device))\n",
        "\n",
        "        if not self.training:\n",
        "            return sentence_embedding_1, sentence_embedding_2\n",
        "\n",
        "        # sentence_projection_1 = self.projection_MLP(sentence_embedding_1)\n",
        "        # sentence_projection_2 = self.projection_MLP(sentence_embedding_2)\n",
        "\n",
        "        return sentence_embedding_1, sentence_embedding_2\n",
        "\n",
        "    def update_queue(self, projections):\n",
        "        projections = torch.nn.functional.normalize(projections, p=2, dim=-1)\n",
        "        self.feature_queue = torch.concat([projections, self.feature_queue[:-config['BATCH_SIZE']]], axis=0)\n",
        "\n",
        "    def mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    def find_nearest_neighbors(self, projections):\n",
        "        projections = torch.nn.functional.normalize(projections, p=2, dim=-1)\n",
        "        support_similarities = torch.matmul(projections, self.feature_queue.T)\n",
        "        # nn_projections = torch.gather(self.feature_queue, dim=0, index=torch.argmax(support_similarities, dim=1).unsqueeze(-1))\n",
        "        # return projections + (nn_projections - projections).detach()\n",
        "        indices = torch.argmax(support_similarities, dim=-1)\n",
        "        return projections + (self.feature_queue[indices] - projections).detach()"
      ],
      "metadata": {
        "id": "U9hdIVGr7Ruw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network(768, 2048, 2048, 2048, config['mixup_rate'], False).to(device)"
      ],
      "metadata": {
        "id": "jy807WrGa2Uk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(model, x1, None)\n",
        "# del model"
      ],
      "metadata": {
        "id": "06Xho9K-9SSb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "criterion = NTXENT(config['temperature'], config['LARGE_NUM'], config['hidden_norm'])"
      ],
      "metadata": {
        "id": "QgBE_wdoa4iO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import wandb\n",
        "\n",
        "# wandb.login(key=\"1b4d95ae47c5d409e738db12aab18c42676f9367\")\n",
        "\n",
        "# run = wandb.init(\n",
        "#     name = \"setup_8\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "#     reinit=True, ### Allows reinitalizing runs when you re-run this cell\n",
        "#     project=\"11711_hw4_NNCLR\", ### Project should be created in your wandb account \n",
        "#     config=config ### Wandb Config for your run\n",
        "# )"
      ],
      "metadata": {
        "id": "JobMOR4kpuEG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "HXB2rwmw48a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    batch_bar = tqdm(total=len(train_dataloader), dynamic_ncols=True, position=0, leave=False, desc='Train')\n",
        "    total_loss = 0\n",
        "    for i, x in enumerate(train_dataloader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # projection1, projection2, prediction1, prediction2 = model(x, None)\n",
        "        projection1, projection2 = model(x, None)\n",
        "        if epoch == 1 and i <= 100:\n",
        "            loss = criterion(projection1, projection2)\n",
        "        else:\n",
        "            neighbor1 = model.find_nearest_neighbors(projection1)\n",
        "            neighbor2 = model.find_nearest_neighbors(projection2)\n",
        "\n",
        "            loss = criterion(neighbor1, projection2) / 2 + criterion(neighbor2, projection1) / 2\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        model.update_queue(projection1)\n",
        "        model.update_queue(projection2)\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            epoch=\"{:d}\".format(epoch),\n",
        "            loss=\"{:.04f}\".format(loss.item()))\n",
        "        batch_bar.update()\n",
        "\n",
        "        if i % 100 == 0 and i != 0:\n",
        "            epc, esc, epd, esd = val()\n",
        "            print(\"Epoch number: \", epoch)\n",
        "            print(\"Pearson Cosine:\", epc)\n",
        "            print(\"Spearman Cosine:\", esc)\n",
        "            print(\"Pearson Dot:\", epd)\n",
        "            print(\"Spearman Dot:\", esd)\n",
        "            print()\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "def val():\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "    total_loss = 0\n",
        "    total_eval_pearson_cosine = 0\n",
        "    total_eval_spearman_cosine = 0\n",
        "    total_eval_pearson_dot = 0\n",
        "    total_eval_spearman_dot = 0\n",
        "    for i, (x1, x2, labels) in enumerate(val_dataloader):\n",
        "        with torch.no_grad():\n",
        "            embedding1, embedding2 = model(x1, x2)\n",
        "\n",
        "        embedding1 = embedding1.detach().cpu().numpy()\n",
        "        embedding2 = embedding2.detach().cpu().numpy()\n",
        "\n",
        "        cosine_scores = [1-i for i in paired_cosine_distances(embedding1, embedding2)]\n",
        "        dot_products = [np.dot(emb1, emb2) for emb1, emb2 in zip(embedding1, embedding2)]\n",
        "\n",
        "        eval_pearson_cosine, _ = pearsonr(np.array(labels), cosine_scores)\n",
        "        eval_spearman_cosine, _ = spearmanr(np.array(labels), cosine_scores)\n",
        "        total_eval_pearson_cosine += eval_pearson_cosine\n",
        "        total_eval_spearman_cosine += eval_spearman_cosine\n",
        "\n",
        "        eval_pearson_dot, _ = pearsonr(np.array(labels), dot_products)\n",
        "        eval_spearman_dot, _ = spearmanr(np.array(labels), dot_products)\n",
        "        total_eval_pearson_dot += eval_pearson_dot\n",
        "        total_eval_spearman_dot += eval_spearman_dot\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            eval_pearson_cosine=\"{:2f}\".format(eval_pearson_cosine),\n",
        "            eval_spearman_cosine=\"{:2f}\".format(eval_spearman_cosine),\n",
        "            eval_pearson_dot=\"{:2f}\".format(eval_pearson_dot),\n",
        "            eval_spearman_dot=\"{:2f}\".format(eval_spearman_dot)\n",
        "        )\n",
        "        batch_bar.update()\n",
        "    return total_eval_pearson_cosine / len(val_dataloader), total_eval_spearman_cosine / len(val_dataloader), total_eval_pearson_dot / len(val_dataloader), total_eval_spearman_dot / len(val_dataloader)"
      ],
      "metadata": {
        "id": "HLibyoU0-O6S"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = config['total_epochs']\n",
        "# scaler = scaler = torch.cuda.amp.GradScaler()\n",
        "# scheduler = ReduceLROnPlateau(optimizer, patience = 5, factor = 0.5, threshold = 0.05, verbose=True)\n",
        "for i in range(1, num_epochs + 1):\n",
        "    train_loss = train(i)\n",
        "\n",
        "    # wandb.log({\"train_loss\": train_loss})\n",
        "    # wandb.save(\"checkpoint\" + str(i) + \".pth\")\n",
        "\n",
        "    print(\"Train loss is \", train_loss)\n",
        "\n",
        "# run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H1o5GWIczyj",
        "outputId": "a5a5d8bf-41ce-44e5-e760-d1ec25c65398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number:  1\n",
            "Pearson Cosine: 0.7248180430962432\n",
            "Spearman Cosine: 0.723587082060752\n",
            "Pearson Dot: 0.6807508982086413\n",
            "Spearman Dot: 0.6757662367191649\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number:  1\n",
            "Pearson Cosine: 0.6961125592016473\n",
            "Spearman Cosine: 0.6865922784314225\n",
            "Pearson Dot: 0.6739447808345315\n",
            "Spearman Dot: 0.6655508416760413\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number:  1\n",
            "Pearson Cosine: 0.7021303508300724\n",
            "Spearman Cosine: 0.6890953878864094\n",
            "Pearson Dot: 0.6911477059627862\n",
            "Spearman Dot: 0.6769499312161625\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number:  1\n",
            "Pearson Cosine: 0.6852182515091464\n",
            "Spearman Cosine: 0.6731375747715064\n",
            "Pearson Dot: 0.6778032311987471\n",
            "Spearman Dot: 0.6644344058298682\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number:  1\n",
            "Pearson Cosine: 0.6866441574219833\n",
            "Spearman Cosine: 0.6689547359832153\n",
            "Pearson Dot: 0.6801351393679508\n",
            "Spearman Dot: 0.6651205735362945\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  58%|█████▊    | 541/930 [19:17<13:10,  2.03s/it, epoch=1, loss=0.3239]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "v6O0C8usLwji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e63060d8-2748-4a94-fee9-198bfc089c7e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "168"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZQysyORQt3_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}